{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "import numpy as np\n",
        "import datetime\n",
        "import uuid\n",
        "import os\n",
        "\n",
        "import azureml.core\n",
        "from azureml.core import Workspace, Dataset, Datastore, Environment, ScriptRunConfig, Experiment\n",
        "from azureml.core.compute import AmlCompute, ComputeTarget\n",
        "from azureml.core.model import Model, InferenceConfig\n",
        "from azure.storage.blob import BlobServiceClient\n",
        "from azureml.widgets import RunDetails\n",
        "\n",
        "# check core SDK version number\n",
        "print(\"Azure ML SDK Version: \", azureml.core.VERSION)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## AzureML Workspace Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Make sure to replace these with values match your Azure resources \n",
        "subscription_id = \"YOUR_SUBSCRIPTION_ID\"\n",
        "resource_group = \"YOUR_RESOURCE_GROUP_NAME\"\n",
        "workspace_name = \"YOUR_WORKSPACE_NAME\"\n",
        "workspace_region = \"YOUR_WORKSPACE_REGION\"\n",
        "container_registry_name = \"YOUR_CONTAINTER_REGISTRY_NAME\"\n",
        "storage_name = \"YOUR_STORAGE_ACCOUNT_NAME\"\n",
        "blob_datastore_name = \"YOUR_DATASTORE_NAME\"\n",
        "key_vault_name = \"YOUR_KEY_VAULT_NAME\"\n",
        "container_name = \"YOUR_STORAGE_CONTAINER_NAME\"\n",
        "storage_key = \"YOUR_STORAGE_KEY\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Setting up repository locations used in the script\n",
        "scripts = \"../scripts\"\n",
        "train_script = \"train_script.py\"\n",
        "score_script = \"score_script.py\"\n",
        "test_script = \"test_script.py\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "try:\n",
        "    ws = Workspace(subscription_id = subscription_id, resource_group = resource_group, workspace_name = workspace_name)\n",
        "    # Write the details of the workspace to a configuration file to the notebook library\n",
        "    print(\"Workspace configuration succeeded.\")\n",
        "except:\n",
        "    # Create a new workspace using the specified parameters\n",
        "    container_registry = f\"/subscriptions/{subscription_id}/resourceGroups/{resource_group}/providers/Microsoft.ContainerRegistry/registries/{container_registry_name}\"\n",
        "    storage_account = f\"/subscriptions/{subscription_id}/resourceGroups/{resource_group}/providers/Microsoft.Storage/storageAccounts/{storage_name}\"\n",
        "    key_vault = f\"/subscriptions/{subscription_id}/resourceGroups/{resource_group}/providers/Microsoft.KeyVault/vaults/{key_vault_name}\"\n",
        "    \n",
        "    ws = Workspace.create(name = workspace_name,\n",
        "                        subscription_id = subscription_id,\n",
        "                        resource_group = resource_group, \n",
        "                        location = workspace_region,\n",
        "                        create_resource_group = True,\n",
        "                        storage_account=storage_account,\n",
        "                        container_registry=container_registry,\n",
        "                        key_vault=key_vault,\n",
        "                        sku = 'basic',\n",
        "                        exist_ok = True)\n",
        "    ws.get_details()\n",
        "    print(\"Workspace not accessible. Created a new workspace\")\n",
        "    \n",
        "# TODO: Work with environments\n",
        "myenv = Environment.from_pip_requirements(name = \"myenv\",\n",
        "                                        file_path = \"../../pipeline/environment/docker-contexts/python-and-pip/requirements.txt\")\n",
        "myenv.register(workspace=ws)\n",
        "ws.write_config(path=\".azureml\", file_name=\"config.json\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Connect to the workspace\n",
        "ws = Workspace.from_config(path=\".azureml\")\n",
        "print(ws.name, ws.resource_group, ws.location, ws.subscription_id, sep = '\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Create Compute"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# choose a name for your cluster\n",
        "compute_name = \"demo-cpu-cluster\"\n",
        "compute_min_nodes = 0\n",
        "compute_max_nodes = 4\n",
        "\n",
        "# This example uses CPU VM.\n",
        "vm_size = \"STANDARD_D2_V2\"\n",
        "\n",
        "if compute_name in ws.compute_targets:\n",
        "    compute_target = ws.compute_targets[compute_name]\n",
        "    if compute_target and type(compute_target) is AmlCompute:\n",
        "        print(\"Found compute target: \" + compute_name)\n",
        "else:\n",
        "    print(\"Creating new compute target...\")\n",
        "    provisioning_config = AmlCompute.provisioning_configuration(vm_size = vm_size,\n",
        "                                                                min_nodes = compute_min_nodes, \n",
        "                                                                max_nodes = compute_max_nodes,\n",
        "                                                                idle_seconds_before_scaledown=600,\n",
        "                                                                admin_username='compute-cluster-admin',\n",
        "                                                                admin_user_password='compute-cluster-password')\n",
        "\n",
        "    # create the cluster\n",
        "    compute_target = ComputeTarget.create(ws, compute_name, provisioning_config)\n",
        "    \n",
        "    # can poll for a minimum number of nodes and for a specific timeout. \n",
        "    # if no min node count is provided it will use the scale settings for the cluster\n",
        "    compute_target.wait_for_completion(show_output=True, min_node_count=None, timeout_in_minutes=20)\n",
        "    \n",
        "     # For a more detailed view of current AmlCompute status, use get_status()\n",
        "    print(compute_target.get_status().serialize())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Prepare Data for Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "storage_connection = f\"DefaultEndpointsProtocol=https;AccountName={storage_name};AccountKey={storage_key};EndpointSuffix=core.windows.net\"\n",
        "\n",
        "# Placeholder Blob location\n",
        "train_src_loc = f\"https://mlopsdemodatalake.blob.core.windows.net/{container_name}/mnist_train.csv\"\n",
        "test_src_loc = f\"https://mlopsdemodatalake.blob.core.windows.net/{container_name}/mnist_test.csv\"\n",
        "\n",
        "# Copying the placeholder blobs over to a newly created directory for this run\n",
        "now = datetime.datetime.now()\n",
        "folder = f'{now.year}/week{now.isocalendar()[1]}/day{now.weekday()}/{uuid.uuid4()}'\n",
        "print(f\"Copy blobs for this run into: {folder}\")\n",
        "train_dest_loc = f'{folder}/mnist_train.csv'\n",
        "test_dest_loc =  f'{folder}/mnist_test.csv' \n",
        "\n",
        "blob_service_client = BlobServiceClient.from_connection_string(storage_connection)\n",
        "train_blob_client = blob_service_client.get_blob_client(container=container_name, blob=train_dest_loc)\n",
        "test_blob_client = blob_service_client.get_blob_client(container=container_name, blob=test_dest_loc)\n",
        "\n",
        "train_copy = train_blob_client.start_copy_from_url(train_src_loc, requires_sync=True)\n",
        "test_copy = test_blob_client.start_copy_from_url(test_src_loc, requires_sync=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create or use a Datastore\n",
        "try:\n",
        "    ds = Datastore.get(ws, blob_datastore_name)\n",
        "    print(\"Found Blob Datastore with name: %s\" % blob_datastore_name)\n",
        "except:\n",
        "    # Create a new datastore using the specified parameters\n",
        "    ds = Datastore.register_azure_blob_container(\n",
        "           workspace=ws,\n",
        "           datastore_name=blob_datastore_name,\n",
        "           account_name=storage_name, \n",
        "           container_name=container_name,\n",
        "           account_key=storage_key)\n",
        "    print(\"Registered blob datastore with name: %s\" % blob_datastore_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create Datasets from the Datastore\n",
        "train = Dataset.Tabular.from_delimited_files(path=(ds, train_dest_loc))\n",
        "test = Dataset.Tabular.from_delimited_files(path=(ds, test_dest_loc))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# Register the Datasets in AzureML\n",
        "print(f\"Register new datasets for versioning\")\n",
        "train.register(workspace=ws, name='demo_train_set_mnist', description='training dataset', create_new_version=True)\n",
        "test.register(workspace=ws, name='demo_test_set_mnist', description='testing dataset', create_new_version=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Create training script "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_script_loc = os.path.join(scripts, train_script)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%writefile $train_script_loc\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import joblib\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from azureml.core import Dataset, Run\n",
        "\n",
        "# Workspace and Run configuration\n",
        "run = Run.get_context()\n",
        "ws = run.experiment.workspace\n",
        "\n",
        "#Import data here!\n",
        "train = Dataset.get_by_name(name='demo_train_set_mnist', workspace=ws, version='latest')\n",
        "test = Dataset.get_by_name(name='demo_test_set_mnist', workspace=ws, version='latest')\n",
        "\n",
        "# Data preparation: Separate labels from features, and normalize features on the fly\n",
        "train_df = train.to_pandas_dataframe()\n",
        "test_df = test.to_pandas_dataframe()  \n",
        "x_train = train_df.iloc[:, 1:] / 255\n",
        "y_train = train_df.loc[:,\"label\"]\n",
        "x_test = test_df.iloc[:, 1:] / 255\n",
        "y_test = test_df.loc[:, \"label\"]\n",
        "\n",
        "print(f'Training set dimension: {x_train.shape, y_train.shape}, Test set dimension: {x_test.shape, y_test.shape}')\n",
        "\n",
        "reg = 0.5\n",
        "print('Train a logistic regression model with regularization rate of', reg)\n",
        "clf = LogisticRegression(C=1.0/reg, solver=\"liblinear\", multi_class=\"auto\", random_state=42)\n",
        "clf.fit(x_train, y_train)\n",
        "\n",
        "print('Predict the test set')\n",
        "y_hat = clf.predict(x_test)\n",
        "\n",
        "# calculate accuracy on the prediction\n",
        "acc = np.average(y_hat == y_test)\n",
        "print('Accuracy is', acc)\n",
        "\n",
        "run.log('regularization rate', np.float(reg))\n",
        "run.log('accuracy', np.float(acc))\n",
        "\n",
        "os.makedirs('outputs', exist_ok=True)\n",
        "# note file saved in the outputs folder is automatically uploaded into experiment record\n",
        "joblib.dump(value=clf, filename='outputs/sklearn_mnist_model.pkl')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Configure and submit the training job"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "src = ScriptRunConfig(source_directory=scripts,\n",
        "                      script=train_script,\n",
        "                      arguments=['--input-data', train.as_named_input('training_dataset'), test.as_named_input('testing_dataset')],\n",
        "                      compute_target=compute_target,\n",
        "                      environment=myenv)\n",
        "\n",
        "experiment_name = 'demo-sklearn-mnist-exp'\n",
        "exp = Experiment(workspace=ws, name=experiment_name)\n",
        "run = exp.submit(config=src)\n",
        "run\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Display Run details"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "RunDetails(run).show()\n",
        "run.wait_for_completion(show_output=True) \n",
        "print(run.get_metrics())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Register the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(run.get_file_names())\n",
        "model = run.register_model(model_name='demo-sklearn-mnist', model_path='outputs/sklearn_mnist_model.pkl', datasets =[('training_data', train), ('testing_data', test)])\n",
        "print(model.name, model.id, model.version, sep='\\t')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Create a scoring script to make predictions using the Container"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "score_script_loc = os.path.join(scripts, score_script)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%writefile $score_script_loc\n",
        "\n",
        "import os\n",
        "import logging\n",
        "import json\n",
        "import numpy\n",
        "import joblib\n",
        "\n",
        "def init():\n",
        "    \"\"\"\n",
        "    This function is called when the container is initialized/started, typically after create/update of the deployment.\n",
        "    You can write the logic here to perform init operations like caching the model in memory\n",
        "    \"\"\"\n",
        "    global model\n",
        "    # AZUREML_MODEL_DIR is an environment variable created during deployment.\n",
        "    # It is the path to the model folder (./azureml-models/$MODEL_NAME/$VERSION)\n",
        "    model_path = os.path.join(\n",
        "        os.getenv(\"AZUREML_MODEL_DIR\"), 'sklearn_mnist_model.pkl'\n",
        "    )\n",
        "    # deserialize the model file back into a sklearn model\n",
        "    model = joblib.load(model_path)\n",
        "    logging.info(\"Init complete\")\n",
        "\n",
        "\n",
        "def run(raw_data):\n",
        "    \"\"\"\n",
        "    This function is called for every invocation of the endpoint to perform the actual scoring/prediction.\n",
        "    In the example we extract the data from the json input and call the scikit-learn model's predict()\n",
        "    method and return the result back\n",
        "    \"\"\"\n",
        "    logging.info(\"model 1: request received\")\n",
        "    data = json.loads(raw_data)[\"data\"]\n",
        "    data = numpy.array(data)\n",
        "    result = model.predict(data)\n",
        "    logging.info(\"Request processed\")\n",
        "    return result.tolist()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Containerize model and publish to ACR"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "inference_config = InferenceConfig(entry_script=score_script_loc, environment=myenv)\n",
        "package = Model.package(ws, [model], inference_config=inference_config, image_name='demo-image-repo', image_label='demo-image')\n",
        "package.wait_for_creation(show_output=True)\n",
        "\n",
        "# Download the package.\n",
        "cont = package.pull()\n",
        "# Get the Azure container registry that the model/Dockerfile uses.\n",
        "acr=package.get_container_registry()\n",
        "print(\"Address:\", acr.address)\n",
        "print(\"Username:\", acr.username)\n",
        "print(\"Password:\", acr.password)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Create a test script for the exported container"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "test_script_loc = os.path.join(scripts, test_script)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%writefile $test_script_loc\n",
        "\n",
        "import requests\n",
        "import json\n",
        "import numpy as np\n",
        "\n",
        "# URL for the web service.\n",
        "scoring_uri = 'http://localhost:6789/score'\n",
        "\n",
        "# Two sets of data to score, so we get two results back.\n",
        "data = {\"data\":\n",
        "        [\n",
        "            np.zeros(784).tolist(),\n",
        "            np.ones(784).tolist(),\n",
        "        ]\n",
        "        }\n",
        "# Convert to JSON string.\n",
        "input_data = json.dumps(data)\n",
        "\n",
        "# Set the content type.\n",
        "headers = {'Content-Type': 'application/json'}\n",
        "\n",
        "# Make the request and display the response.\n",
        "resp = requests.post(scoring_uri, input_data, headers=headers)\n",
        "print(resp.text)"
      ]
    }
  ],
  "metadata": {
    "interpreter": {
      "hash": "5dbe7d935004e9310a46bd29cb5bbb409a040f22b05fa1ad82418ef8c081a09c"
    },
    "kernel_info": {
      "name": "python3"
    },
    "kernelspec": {
      "display_name": "Python 3.9.0 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.0"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
